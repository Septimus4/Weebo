name: local-llm-assistant
version: 1.0.0
schema: v1

environment:
  - CUDA_VISIBLE_DEVICES=0
  - OLLAMA_KEEP_ALIVE=600

models:
  # Chat & general editing
  - name: qwen-chat
    provider: ollama
    model: qwen3:30b
    roles:
      - chat

  # Fast code completion with FIM support
  - name: qwen-code
    provider: ollama
    model: qwen2.5-coder:14b
    roles:
      - autocomplete
    defaultCompletionOptions:
      contextLength: 8112
      maxTokens: 512
      temperature: 0.0
      topP: 0.95

  # Slow code edit, apply
  - name: qwen-code
    provider: ollama
    model: qwen2.5-coder:14b
    roles:
      - edit
      - apply
    defaultCompletionOptions:
      contextLength: 32768
      maxTokens: 32768

  # Text embeddings for vector search
  - name: nomic-embed
    provider: ollama
    model: nomic-embed-text:latest
    roles:
      - embed

context:
  - provider: code
  - provider: currentFile
  - provider: docs
  - provider: diff
  - provider: terminal
  - provider: problems
  - provider: folder
  - provider: codebase
  - provider: url
  - provider: web
    params:
      n: 5
  - provider: commit
    params:
      Depth: 50
      LastXCommitsDepth: 10
